{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2023-02-02T19:58:26.687535Z","iopub.execute_input":"2023-02-02T19:58:26.688884Z","iopub.status.idle":"2023-02-02T19:58:26.704153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![bellabeat logo](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRLX-tdij_MQ7lDd2bjJxA5BGQra7ELOWAJklDKmQTE&s)","metadata":{}},{"cell_type":"markdown","source":"About the company:\n\nBellabeat is a technology company focused on wearable health-centric products for women. Bellabeat has found success and is looking for growth to become a power-player in the global smart device market. Founders Urška Sršen and Sando Mur, designed their technology to inform and empower women around the world to be more cognisant about their health and habits. Bellabeat wearables collect data on activity, sleep, stress, and reporductivity health. In addition to diffrent wearable options, these trackers are accompanied by the Bellabeat app to provide feedback and help users understand the data and their habits. Lastly, they even have a waterbottle which utilizes smart technology to track user water intake to make sure they’re properly hydrated. \n\nGuiding Questions from stakeholders:\n\nWhat are some trends in smart device usage?\nHow could these trends apply to Bellabeat customers?\nHow could these trends help influence Bellabeat marketing strategy?\n\nAsk:\n\nGuiding Questions for Analysis:\n1. What is the problem you are trying to solve?\nThe problem we are trying to solve is to analyze smart device usage data and how to use that data to drive new business decisions.\n2. How can your insights drive business decisions \nWe can take the insights from our analysis to recommended actual data-backed suggestions on how to grow and improve Bellabeat.\n\nKey Tasks: \n1. Identify the business task\nThe business task is as stated above. To analyze usage data and report our finding with recommendations on improvements to the stakeholders.\n2. Consider key stakeholders\nUrska Srsen: Co-founder and CCO\nSando Mur: Co-founder and Mathematician (executive team member)\nDeliverable:\n1. Clear statement of business task\nAnalyze fitbit tracker data to gain insight into usage trends and how those trends can be leveraged by the marketing team to identify new growth opportunities. It is important to note that this tracker data is not first-party from Bellabeat consumers. This is third-party data analyzed from a rival smart device called FitBit. Gaining insights from other competitor’s wearables can help Bellabeat with new strategies. \n\nPrepare: \n\nGuiding Questions:\n1. Where is your data stored?\nThis data is stored on kaggle and was made by Mobius\n2. How is the data organized? Is it long or wide format?\nThe data is organized in long format and consists of 18 .csv files in total\n3. Are there issues with bias or credibility in this data? Does your data ROCCC?\nReliable - LOW this dataset only has a sample size of 30 which is the lowest recommended sample size. There are also many other unknown factors such as age, gender, and height.\nOriginal - LOW this is third party data collected by Amazon Mechanical Turk between 03.12.2016-05.12.2016\nComprehensive - MEDIUM data contains a variety of variable including minute-level output for physical activity, heart rate, and sleep monitoring\nCurrent - LOW data collected is from almost 7 years ago, a lot about a person’s lifestyle can change in that time \nCited - HIGH the source and data collected is well documented\n4. How are you addressing licensing, privacy, security, and accessibility?\nThis data is CC0: Public domain so it is free to use for the public\n5. How did you verify the data’s integrity?\nThe data was easily accessible, transferable, and has a 10/10 usability score from Kaggle assuring its integrity\n6. How does it help you answer your question?\nThis dataset will provide insight into the usage patterns of FitBit wearers\n7. Are there any problems with the data?\nAt first glance there are no apparent problems with the data.\n\nKey Tasks:\n1. Download data and store it appropriately\nThe data was downloaded and stored in a folder named “bellabeat_casestudy”. Inside that folder was another name “FitBit Data 4.12.16-5.12.16” which holds the 18 .csv files we can use.\n2. Identify how it’s organized \nData is organized by scale: minute, hourly, daily and by type: activity, sleep, weight, etc.\n3. Sort and filter the data\n4. Determine the credibility of the data\n\nDeliverables:\n1. A description of all the data sources used\nI will use Microsoft Excel for data cleaning\n\nProcess:\n\nGuiding questions:\n\n1. What tools are you choosing and why?  \n\n - The tools I am choosing are Excel and R. I chose Excel because the datasets we’re working with are not large enough to warrant SQL. I am using R for my analysis because it has both data cleaning and visualization capabilities within its platform.\n\n2. Have you ensured your data’s integrity? \n\n - Yes, I have ensured the data’s integrity by using conditional formatting to make sure all of the values make sense in the associated columns. For example, I made sure that there were no negative values because that wouldn’t make sense given our column names and what we’re analyzing. \n\n3. What steps have you taken to ensure that your data is clean? \n\n - I utilized conditional formatting in Excel to make sure there are no missing values, and if there were any, to make note of it. Again, I also checked to see if there were any negative values.\n\n4. How can you verify that your data is clean and ready to analyze? \n\n - See the steps I have taken above. \n\n5. Have you documented your cleaning process so you can review and share those results?\n\n - Yes, see below.\n\nData Cleaning:\n - Used excel to clean datasets since the sample size is small\n - Found duplicate data using conditional formatting in multiple sheets with redundant data, so I wont be using them moving forward in my analysis.\n - Total_distance and tracker_distance repeat the same values so i will be deleting the column tracker_distance\n - There is no associated metric with distance…is it miles or kilometers?\n - Daily activity contains all of the data also found in daily_steps and daily_intensities, so for simplicity sake I will just be using the daily_activity sheet\n - The other two sheets I will be using in my analysis are weight_log and daily_sleep\n - In daily_sleep there are only 8 unique user_ids with data, many of which manually inputted the data, and many of which did not record data everyday in the observed time period. Due to such a small sample size, I won’t be able to gather any statistically significant findings using this data.","metadata":{}},{"cell_type":"markdown","source":"Analyze: \n\n - Here is my analysis in R including the code chunks and their outputs.","metadata":{}},{"cell_type":"code","source":"library(tidyverse)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T19:58:26.706024Z","iopub.execute_input":"2023-02-02T19:58:26.707133Z","iopub.status.idle":"2023-02-02T19:58:26.722016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading Datasets to be Used:**\n\nHere we're going to load the three data sets that we'll be using in our analysis, which we'll call daily_activity, daily_sleep, and weight_log. ","metadata":{}},{"cell_type":"code","source":"daily_activity <- read.csv(\"/kaggle/input/bellabeat-analysis/daily_activity_cleaned.csv\")\ndaily_sleep <- read.csv(\"/kaggle/input/bellabeat-analysis/daily_sleep_cleaned.csv\")\nweight_log <- read.csv(\"/kaggle/input/bellabeat-analysis/weight_log_cleaned.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-02-02T20:03:58.927887Z","iopub.execute_input":"2023-02-02T20:03:58.929272Z","iopub.status.idle":"2023-02-02T20:03:58.97421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploring Key Tables:**\n\nLet's preview all of our datasets to make sure they imported properly.","metadata":{}},{"cell_type":"code","source":"head(daily_activity)\nhead(daily_sleep)\nhead(weight_log)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T20:04:40.97523Z","iopub.execute_input":"2023-02-02T20:04:40.976535Z","iopub.status.idle":"2023-02-02T20:04:41.054916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's see the column names in our datasets.","metadata":{}},{"cell_type":"code","source":"colnames(daily_activity)\ncolnames(daily_sleep)\ncolnames(weight_log)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T20:11:32.603622Z","iopub.execute_input":"2023-02-02T20:11:32.604864Z","iopub.status.idle":"2023-02-02T20:11:32.626143Z"},"trusted":true},"execution_count":null,"outputs":[]}]}